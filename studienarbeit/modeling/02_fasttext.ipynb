{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling using FastText\n",
    "\n",
    "FastText is a approach by facebook to create word embeddings. It is based on the word2vec approach, but it is able to handle out of vocabulary words. It is also able to handle subwords. This means that it is able to handle words that are not in the vocabulary, but are made up of subwords that are in the vocabulary. For example, the word \"university\" is not in the vocabulary, but the word \"univers\" is. FastText is able to handle this by using the subword \"univers\" to create a vector for the word \"university\".\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from studienarbeit.config import party_encoding\n",
    "from studienarbeit.utils.load import EDataTypes, Load\n",
    "\n",
    "# Either load the bin file using the command, or the text (vector) file from https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "fasttext.util.download_model(\"de\", if_exists=\"ignore\")\n",
    "\n",
    "sns.set(style=\"white\", palette=\"muted\", rc={\"figure.figsize\": (20, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "file_name = \"prep_tweets_full.parquet\"\n",
    "data_type = EDataTypes.TWEETS\n",
    "data_dir = Path(\"../../data/\") / data_type.value\n",
    "\n",
    "suffix = []\n",
    "\n",
    "if \"sent\" in file_name:\n",
    "    suffix.append(\"sent\")\n",
    "\n",
    "if \"full\" in file_name:\n",
    "    suffix.append(\"full\")\n",
    "elif \"sm\" in file_name:\n",
    "    suffix.append(\"sm\")\n",
    "elif \"md\" in file_name:\n",
    "    suffix.append(\"md\")\n",
    "elif \"lg\" in file_name:\n",
    "    suffix.append(\"lg\")\n",
    "\n",
    "load = Load(data_type=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load.load_dataframe(file_name, columns=[\"clean_text\", \"lemma_text\", \"filter_text\", \"party\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"filter_text\"], df[\"party\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir / f\"cache/train_{'_'.join(suffix)}.txt\", \"w\") as f:\n",
    "    for index, row in pd.DataFrame({\"text\": X_train, \"party\": y_train}).iterrows():\n",
    "        f.write(f\"__label__{row['party']} {row['text']}\\n\")\n",
    "\n",
    "with open(data_dir / f\"cache/test_{'_'.join(suffix)}.txt\", \"w\") as f:\n",
    "    for index, row in pd.DataFrame({\"text\": X_test, \"party\": y_test}).iterrows():\n",
    "        f.write(f\"__label__{row['party']} {row['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(\n",
    "    input=str(data_dir / f\"cache/train_{'_'.join(suffix)}.txt\"),\n",
    "    epoch=5,\n",
    "    lr=0.1,\n",
    "    wordNgrams=2,\n",
    "    loss=\"softmax\",\n",
    "    dim=300,\n",
    "    pretrainedVectors=\"cc.de.300.vec\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    model.save_model(str(data_dir / f\"models/fasttext_{'_'.join(suffix)}.bin\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.test(path=str(data_dir / f\"cache/test_{'_'.join(suffix)}.txt\"))\n",
    "\n",
    "print(f\"Count of test data (N): {test_score[0]}\")\n",
    "print(f\"F1 Score: {2 * ((test_score[1] * test_score[2]) / (test_score[1] + test_score[2]))}\")\n",
    "print(f\"Percision: {test_score[1]}\")\n",
    "print(f\"Recall: {test_score[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"text\": X_test, \"party\": y_test})\n",
    "\n",
    "df_test[\"prediction\"] = df_test[\"text\"].apply(lambda x: int(model.predict(x)[0][0].replace(\"__label__\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_test[\"party\"], df_test[\"prediction\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konfusionsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_test[\"party\"], df_test[\"prediction\"], normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=party_encoding.keys())\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[df_test[\"party\"] != df_test[\"prediction\"]].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(model, words):\n",
    "    word_vectors = np.array([model.get_word_vector(w) for w in words])\n",
    "    threedim = PCA().fit_transform(word_vectors)[:, :3]\n",
    "    scatter = go.Scatter3d(x=threedim[:, 0], y=threedim[:, 1], z=threedim[:, 2], mode=\"markers\", text=words)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=\"3D PCA\",\n",
    "        showlegend=True,\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"PC1\"),\n",
    "            yaxis=dict(title=\"PC2\"),\n",
    "            zaxis=dict(title=\"PC3\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plot_figure = go.Figure(data=scatter, layout=layout)\n",
    "    plot_figure.show()\n",
    "\n",
    "\n",
    "plot_pca(\n",
    "    model,\n",
    "    [\"afd\", \"weidel\", \"spd\", \"sozial\", \"grüne\", \"grünen\", \"union\", \"cdu\", \"csu\", \"linke\", \"linke\", \"fdp\", \"steuern\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7abe641ef5c60e7b2b79f06dad82c6b1ae6b3c4f8500bc012ee8285dc22561c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
