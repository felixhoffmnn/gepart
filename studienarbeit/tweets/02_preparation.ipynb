{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Tweets\n",
    "\n",
    "In this notebook we will preprocess the tweets. We will remove the stopwords, the punctuation and the links. We will also lemmatize the words.\n",
    "\n",
    "If you are looking for the initial understanding please have a look at [01_understanding.ipynb](01_understanding.ipynb). For the visual exploration after the cleaning please have a look at [03_plotting.ipynb](03_plotting.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from nltk import ngrams\n",
    "from tqdm import tqdm\n",
    "\n",
    "from studienarbeit.utils.cleaning import Cleaning\n",
    "from studienarbeit.utils.load import EDataTypes, Load\n",
    "from studienarbeit.utils.sentiment import Sentiment\n",
    "\n",
    "load_dotenv()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If true, it will try to load the cached dataframe otherwise it will process the data\n",
    "FAST_MODE = True\n",
    "# If true, it will run the sentiment analysis\n",
    "SENTIMENT_ANALYSIS = True\n",
    "# Set to a number of tweets to limit amount of data needed to process, set to None to process all data\n",
    "SAMPLE_SIZE = None\n",
    "\n",
    "file_name = \"tweets_understanding.parquet\"\n",
    "data_type = EDataTypes.TWEETS\n",
    "data_dir = Path(\"../../data\")\n",
    "\n",
    "with open(data_dir / \"gender_encoding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gender_encoding = json.load(f)\n",
    "with open(data_dir / \"party_encoding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    party_encoding = json.load(f)\n",
    "    \n",
    "load = Load(data_type=data_type)\n",
    "clean = Cleaning()\n",
    "sentiment = Sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load.load_dataframe(file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pipeline(df: pd.DataFrame, min_word_count: int = 5):\n",
    "    if FAST_MODE:\n",
    "        logger.info(\"Fast mode is enabled, skipping sentiment analysis...\")\n",
    "    \n",
    "    # Group CDU and CSU as Union\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CSU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CDU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].cat.remove_unused_categories()\n",
    "\n",
    "    # Fix labels for retweets\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"FALSE\", False)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"TRUE\", True)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].astype(\"bool\")\n",
    "\n",
    "    # Remove tweets from parties that are not in the Bundestag and/or retweets\n",
    "    print(\n",
    "        f\"The dataset contains {len(df.loc[(df['is_retweet'] == True) | (df['text'].str.startswith('RT'))])} retweets...\"\n",
    "    )\n",
    "    df = df.loc[(df[\"party\"] != \"Parteilos\") & (df[\"is_retweet\"] == False) & (~df[\"text\"].str.startswith(\"RT\"))]\n",
    "\n",
    "    # Encode party and gender\n",
    "    df[\"party\"] = df[\"party\"].map(party_encoding).astype(\"int8\")\n",
    "    df[\"gender\"] = df[\"gender\"].map(gender_encoding).astype(\"int8\")\n",
    "\n",
    "    # Apply cleaning pipeline\n",
    "    df[\"clean_text\"] = df[\"text\"].progress_apply(clean.clean_text).astype(\"string[pyarrow]\")\n",
    "    df[\"lemma_text\"] = df[\"clean_text\"].progress_apply(clean.lemma_text).astype(\"string[pyarrow]\")\n",
    "    df[\"filter_text\"] = df[\"lemma_text\"].progress_apply(clean.filter_text).astype(\"string[pyarrow]\")\n",
    "\n",
    "    # Count the number of words and tokens in the tweet\n",
    "    df[\"init_word_count\"] = df[\"text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"init_symbol_count\"] = df[\"text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "    df[\"clean_word_count\"] = df[\"clean_text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"clean_symbol_count\"] = df[\"clean_text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "    df[\"filter_word_count\"] = df[\"filter_text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"filter_symbol_count\"] = df[\"filter_text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "\n",
    "    # Filter out tweets that are too short\n",
    "    print(\n",
    "        f\"Found {len(df.loc[df['filter_word_count'] < min_word_count])} tweets with less than {min_word_count} words...\"\n",
    "    )\n",
    "    df = df.loc[df[\"filter_word_count\"] >= min_word_count]\n",
    "\n",
    "    # Calculate the sentiment of the tweets\n",
    "    if SENTIMENT_ANALYSIS:\n",
    "        df[\"sentiment\"] = df[\"clean_text\"].progress_apply(sentiment.predict_sentiment).astype(\"category\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either load the cached data or process the raw tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = []\n",
    "\n",
    "if SENTIMENT_ANALYSIS:\n",
    "    suffix.append(\"sent\")\n",
    "\n",
    "if SAMPLE_SIZE is None:\n",
    "    suffix.append(\"full\")\n",
    "elif SAMPLE_SIZE <= 25000:\n",
    "    suffix.append(\"sm\")\n",
    "elif SAMPLE_SIZE <= 50000:\n",
    "    suffix.append(\"md\")\n",
    "elif SAMPLE_SIZE <= 100000:\n",
    "    suffix.append(\"lg\")\n",
    "\n",
    "file_path = f\"prep_tweets_{'_'.join(suffix)}.parquet\"\n",
    "\n",
    "if FAST_MODE and load.check_file_exists(file_path):\n",
    "    df_prep = load.load_dataframe(file_path)\n",
    "else:\n",
    "    df_prep = prep_pipeline(df.sample(SAMPLE_SIZE, random_state=42).copy() if SAMPLE_SIZE else df.copy())\n",
    "    load.save_dataframe(df_prep, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test purposes\n",
    "# TODO: Remove this after rerunning the dataframes\n",
    "df_prep[\"init_word_count\"] = df_prep[\"text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "df_prep[\"init_symbol_count\"] = df_prep[\"text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "df_prep[\"screen_name\"] = df_prep[\"screen_name\"].astype(\"string[pyarrow]\")\n",
    "df_prep[\"party\"] = df_prep[\"party\"].map({value: key for key, value in party_encoding.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep[\"party\"] = df_prep[\"party\"].replace(\"DIE GRÜNEN\", \"Grüne\").replace(\"DIE LINKE\", \"Linke\").replace(\"UNION\", \"Union\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following you find our tweet example for visualizing the data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.pipeline(\n",
    "    \"Ehemalige @AfD-Vorsitzende #Petry muss wegen Meineid vor Gericht. Kein Einzelfall: gegen circa 10% aller AfD-Abgeordneten bundesweit laufen oder liefen Strafverfahren. Kriminelle Asylbewerber? Fehlanzeige. Kriminelle AfD-Hetzer trifft den Nagel eher auf den Kopf <U+0001F602> #AfD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique users\n",
    "df_prep[\"screen_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of politicians per party\n",
    "df_prep.groupby(\"party\")[\"screen_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tweets by a individual politician grouped by party\n",
    "df_prep.groupby(\"party\")[\"screen_name\"].value_counts().groupby(\"party\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for n-grams\n",
    "Counter(\n",
    "    list(itertools.chain.from_iterable(df_prep[\"filter_text\"].str.split().apply(lambda x: list(ngrams(x, 3)))))\n",
    ").most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.info(verbose=True, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7abe641ef5c60e7b2b79f06dad82c6b1ae6b3c4f8500bc012ee8285dc22561c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
