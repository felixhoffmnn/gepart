{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets (Sältzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from studienarbeit.utils.plots import Plots\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_MODE = False\n",
    "\n",
    "data_dir = Path(\"../../data/tweets\")\n",
    "party_encoding = {\n",
    "    \"AfD\": 0,\n",
    "    \"FDP\": 1,\n",
    "    \"DIE GRÜNEN\": 2,\n",
    "    \"DIE LINKE\": 3,\n",
    "    \"SPD\": 4,\n",
    "    \"UNION\": 5,\n",
    "}\n",
    "gender_encoding = {\n",
    "    \"male\": 0,\n",
    "    \"female\": 1,\n",
    "}\n",
    "\n",
    "plot = Plots(document_type=\"Tweets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "---\n",
    "\n",
    "Lorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_parquet(\n",
    "    data_dir / \"tweets.parquet\",\n",
    "    columns=[\"screen_name\", \"created_at\", \"is_retweet\", \"text\", \"party\", \"birthyear\", \"gender\"],\n",
    "    use_nullable_dtypes=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_base.columns:\n",
    "    df_base[col] = df_base[col].apply(\n",
    "        lambda x: None if x == \"\" or x == \"NA\" or x == \"NA, NA\" or x == \"NA, NA, NA, NA, NA, NA, NA, NA\" else x\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we can see that there are about 11k missing values in the `text` column. Regarding the `is_retweet` column, about 3k entries have missing values.\n",
    "\n",
    "Following we will delete the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.dropna(subset=[\"text\", \"is_retweet\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we can check which columns represent categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.nunique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean duplicated rows (some tweets seem to be scraped twice at different days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.drop_duplicates(\n",
    "    subset=[\"screen_name\", \"is_retweet\", \"text\", \"party\", \"birthyear\", \"gender\"], keep=\"last\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[\"party\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.groupby(\"gender\")[\"screen_name\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    \"screen_name\": \"category\",\n",
    "    \"created_at\": \"datetime64[ns]\",\n",
    "    \"is_retweet\": \"category\",\n",
    "    \"text\": \"string[pyarrow]\",\n",
    "    \"party\": \"category\",\n",
    "    \"birthyear\": \"datetime64[ns]\",\n",
    "    \"gender\": \"category\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.astype(convert_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.info(verbose=True, memory_usage=\"deep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.describe(include=\"all\", datetime_is_numeric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from nltk import ngrams\n",
    "from studienarbeit.utils.cleaning import Cleaning\n",
    "from studienarbeit.utils.sentiment import Sentiment\n",
    "\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(progress_bar=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = data_dir / \"cache/tweets_prep.parquet\"\n",
    "\n",
    "clean = Cleaning()\n",
    "sentiment = Sentiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.pipeline(\n",
    "    \"Ehemalige @AfD-Vorsitzende #Petry muss wegen Meineid vor Gericht. Kein Einzelfall: gegen circa 10% aller AfD-Abgeordneten bundesweit laufen oder liefen Strafverfahren. Kriminelle Asylbewerber? Fehlanzeige. Kriminelle AfD-Hetzer trifft den Nagel eher auf den Kopf <U+0001F602> #AfD\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pipeline(df: pd.DataFrame, min_word_count: int = 5):\n",
    "    # Group CDU and CSU as Union\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CSU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CDU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].cat.remove_unused_categories()\n",
    "\n",
    "    # Fix labels for retweets\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"FALSE\", False)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"TRUE\", True)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].astype(\"bool\")\n",
    "\n",
    "    # Remove tweets from parties that are not in the Bundestag and/or retweets\n",
    "    print(\n",
    "        f\"The dataset contains {len(df.loc[(df['is_retweet'] == True) | (df['text'].str.startswith('RT'))])} retweets...\"\n",
    "    )\n",
    "    df = df.loc[(df[\"party\"] != \"Parteilos\") & (df[\"is_retweet\"] == False) & (~df[\"text\"].str.startswith(\"RT\"))]\n",
    "\n",
    "    # Encode party and gender\n",
    "    df[\"party\"] = df[\"party\"].map(party_encoding).astype(\"int8\")\n",
    "    df[\"gender\"] = df[\"gender\"].map(gender_encoding).astype(\"int8\")\n",
    "\n",
    "    # Apply cleaning pipeline\n",
    "    df[\"clean_text\"] = df[\"text\"].parallel_apply(clean.clean_text).astype(\"string[pyarrow]\")\n",
    "    df[\"lemma_text\"] = df[\"clean_text\"].parallel_apply(clean.lemma_text).astype(\"string[pyarrow]\")\n",
    "    df[\"filter_text\"] = df[\"lemma_text\"].parallel_apply(clean.filter_text).astype(\"string[pyarrow]\")\n",
    "\n",
    "    # Count the number of words and tokens in the tweet\n",
    "    df[\"clean_word_count\"] = df[\"clean_text\"].parallel_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"clean_symbol_count\"] = df[\"clean_text\"].parallel_apply(lambda x: len(x)).astype(\"int16\")\n",
    "    df[\"filter_word_count\"] = df[\"filter_text\"].parallel_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"filter_symbol_count\"] = df[\"filter_text\"].parallel_apply(lambda x: len(x)).astype(\"int16\")\n",
    "\n",
    "    # Filter out tweets that are too short\n",
    "    print(\n",
    "        f\"Found {len(df.loc[df['lemma_word_count'] < min_word_count])} tweets with less than {min_word_count} words...\"\n",
    "    )\n",
    "    df = df.loc[df[\"lemma_word_count\"] >= min_word_count]\n",
    "\n",
    "    # Calculate the sentiment of the tweets\n",
    "    df[\"sentiment\"] = df[\"clean_text\"].progress_apply(sentiment.predict_sentiment).astype(\"category\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either load the cached data or process the raw tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST_MODE and cache_file.exists():\n",
    "    df_prep = pd.read_parquet(cache_file)\n",
    "else:\n",
    "    df_prep = prep_pipeline(df_base.sample(50000, random_state=42).copy()).reset_index(drop=True)\n",
    "\n",
    "    if not (data_dir / \"cache\").exists():\n",
    "        (data_dir / \"cache\").mkdir()\n",
    "    df_prep.to_parquet(cache_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\n",
    "    list(itertools.chain.from_iterable(df_prep[\"filter_text\"].str.split().apply(lambda x: ngrams(x, 2))))\n",
    ").most_common(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.info(verbose=True, memory_usage=\"deep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.describe(include=\"all\", datetime_is_numeric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = pd.read_parquet(data_dir / \"cache/tweets_prep.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_prep.copy()\n",
    "df_plot[\"party\"] = df_plot[\"party\"].map({value: key for key, value in party_encoding.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.party_count(df_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.sentiment(df_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.word_count(df_plot, \"stemm_word_count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.gender(df_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.user_count(df_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "corr = df_prep.select_dtypes(exclude=[\"object\", \"category\", \"datetime64[ns]\", \"bool\"]).corr(numeric_only=True)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "mask[np.diag_indices_from(mask)] = False\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "corr_plot = sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    "    annot=True,\n",
    "    annot_kws={\"fontsize\": 10},\n",
    "    fmt=\".2f\",\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = df_prep[[\"clean_text\", \"lemma_text\", \"filter_text\", \"party\"]]\n",
    "df_modeling.to_parquet(data_dir / \"cache/tweets_modeling.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = pd.read_parquet(data_dir / \"cache/tweets_modeling.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(sublinear_tf=True, min_df=5, norm=\"l2\", encoding=\"latin-1\", ngram_range=(1, 2))\n",
    "bow_vector = CountVectorizer(ngram_range=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_features = bow_vector.fit_transform(df_modeling[\"filter_text\"])\n",
    "bow_labels = df_modeling[\"party\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = tfidf_vector.fit_transform(df_modeling.filter_text).toarray()\n",
    "tfidf_labels = df_modeling[\"party\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "for party, party_id in sorted(party_encoding.items()):\n",
    "    features_chi2 = chi2(tfidf_features, tfidf_labels == party_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf_vector.get_feature_names_out())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(\" \")) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(\" \")) == 2]\n",
    "    print(f\"# {party}\")\n",
    "    print(f\"\\tMost correlated unigrams: {unigrams[-N:]}\")\n",
    "    print(f\"\\tMost correlated bigrams: {bigrams[-N:]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "# df_cv = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, bow_features, bow_labels, scoring=\"accuracy\", cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "df_cv = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])\n",
    "\n",
    "sns.boxplot(x=\"model_name\", y=\"accuracy\", data=df_cv)\n",
    "sns.stripplot(x=\"model_name\", y=\"accuracy\", data=df_cv, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_modeling[\"filter_text\"], df_modeling[\"party\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(bow_vector.fit_transform(X_train), y_train)\n",
    "cross_val = cross_val_score(svc, bow_vector.transform(X_train), y_train, cv=5)\n",
    "print(f\"Cross validation score: {cross_val.mean():.3f} +/- {cross_val.std():.3f}\")\n",
    "y_pred = svc.predict(bow_vector.transform(X_test))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=party_encoding.keys())\n",
    "disp.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "# df_cv = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, tfidf_features, tfidf_labels, scoring=\"accuracy\", cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "df_cv = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])\n",
    "\n",
    "sns.boxplot(x=\"model_name\", y=\"accuracy\", data=df_cv)\n",
    "sns.stripplot(x=\"model_name\", y=\"accuracy\", data=df_cv, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_modeling[\"filter_text\"], df_modeling[\"party\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(tfidf_vector.fit_transform(X_train), y_train)\n",
    "cross_val = cross_val_score(svc, tfidf_vector.transform(X_train), y_train, cv=5)\n",
    "print(f\"Cross validation score: {cross_val.mean():.3f} +/- {cross_val.std():.3f}\")\n",
    "y_pred = svc.predict(tfidf_vector.transform(X_test))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=party_encoding.keys())\n",
    "disp.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lazypredict.Supervised import LazyClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(tfidf_features, tfidf_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# clf = LazyClassifier(verbose=1, ignore_warnings=True, custom_metric=None)\n",
    "# models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_modeling[\"filter_text\"], df_modeling[\"party\"], test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"text\": X_test, \"party\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_modeling.copy()\n",
    "\n",
    "with open(Path(\"train.txt\"), \"w\") as f:\n",
    "    for index, row in test[:int(test.shape[0] * 0.8)].iterrows():\n",
    "        f.write(f\"__label__{row['party']} {row['filter_text']}\\n\")\n",
    "\n",
    "with open(Path(\"test.txt\"), \"w\") as f:\n",
    "    for index, row in test[int(test.shape[0] * 0.8):].iterrows():\n",
    "        f.write(f\"__label__{row['party']} {row['filter_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    f1 = 2 *((p*r)/(p+r))\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"F1\\t\" + str(f1))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# fasttext.util.download_model(\"de\", if_exists=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\"input\": \"train.txt\", \"pretrainedVectors\": \"cc.de.300.vec\"} # \"epoch\": 50, \"lr\": 0.05, \"wordNgrams\": 2, \"verbose\": 2, \"minCount\":1, \"loss\": \"ns\", \"lrUpdateRate\": 100, \"thread\": 4, \"ws\": 5, \"dim\": 300,\n",
    "model = fasttext.train_supervised(input=\"train.txt\", epoch=5, lr=0.1, wordNgrams=2, loss=\"softmax\", dim=300, pretrainedVectors=\"cc.de.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.test(\"test.txt\")\n",
    "print_results(*test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"] = df_test[\"text\"].apply(lambda x: int(model.predict(x)[0][0].replace(\"__label__\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_test[\"party\"], df_test[\"prediction\"], normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=party_encoding.keys())\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7abe641ef5c60e7b2b79f06dad82c6b1ae6b3c4f8500bc012ee8285dc22561c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
