{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets (SÃ¤ltzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "from studienarbeit.config import gender_encoding, party_encoding\n",
    "from studienarbeit.utils.load import EDataTypes, Load\n",
    "from studienarbeit.utils.plots import Plots\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If true, it will try to load the cached dataframe otherwise it will process the data\n",
    "FAST_MODE = True\n",
    "# If true, it will run the sentiment analysis\n",
    "SENTIMENT_ANALYSIS = True\n",
    "# Set to a number of tweets to limit amount of data needed to process, set to None to process all data\n",
    "SAMPLE_SIZE = None\n",
    "\n",
    "file_name = \"prep_tweets_fast_full.parquet\"\n",
    "data_type = EDataTypes.TWEETS\n",
    "data_dir = Path(\"../../data/\") / data_type.value\n",
    "\n",
    "load = Load(data_type=data_type)\n",
    "plot = Plots(document_type=\"Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = load.load_dataframe(\n",
    "    \"tweets.parquet\", columns=[\"screen_name\", \"created_at\", \"is_retweet\", \"text\", \"party\", \"birthyear\", \"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_base.columns:\n",
    "    df_base[col] = df_base[col].apply(\n",
    "        lambda x: None if x == \"\" or x == \"NA\" or x == \"NA, NA\" or x == \"NA, NA, NA, NA, NA, NA, NA, NA\" else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the count and distribution before any preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape before dropping na: {df_base[df_base['party'] != 'Parteilos'].shape}\")\n",
    "print(f\"\\nParty distribution before preprocessing: \\n{df_base[df_base['party'] != 'Parteilos']['party'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we can see that there are about 11k missing values in the `text` column. Regarding the `is_retweet` column, about 3k entries have missing values.\n",
    "\n",
    "Following we will delete the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.dropna(subset=[\"text\", \"is_retweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we can check which columns represent categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean duplicated rows (some tweets seem to be scraped twice at different days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.drop_duplicates(\n",
    "    subset=[\"screen_name\", \"is_retweet\", \"text\", \"party\", \"birthyear\", \"gender\"], keep=\"last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.groupby(\"gender\")[\"screen_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    \"screen_name\": \"category\",\n",
    "    \"created_at\": \"datetime64[ns]\",\n",
    "    \"is_retweet\": \"category\",\n",
    "    \"text\": \"string[pyarrow]\",\n",
    "    \"party\": \"category\",\n",
    "    \"birthyear\": \"datetime64[ns]\",\n",
    "    \"gender\": \"category\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.info(verbose=True, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import ngrams\n",
    "from tqdm import tqdm\n",
    "\n",
    "from studienarbeit.utils.cleaning import Cleaning\n",
    "from studienarbeit.utils.sentiment import Sentiment\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = Cleaning()\n",
    "sentiment = Sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.pipeline(\n",
    "    \"Ehemalige @AfD-Vorsitzende #Petry muss wegen Meineid vor Gericht. Kein Einzelfall: gegen circa 10% aller AfD-Abgeordneten bundesweit laufen oder liefen Strafverfahren. Kriminelle Asylbewerber? Fehlanzeige. Kriminelle AfD-Hetzer trifft den Nagel eher auf den Kopf <U+0001F602> #AfD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pipeline(df: pd.DataFrame, min_word_count: int = 5):\n",
    "    if FAST_MODE:\n",
    "        logger.info(\"Fast mode is enabled, skipping sentiment analysis...\")\n",
    "    \n",
    "    # Group CDU and CSU as Union\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CSU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].replace(\"CDU\", \"UNION\")\n",
    "    df[\"party\"] = df[\"party\"].cat.remove_unused_categories()\n",
    "\n",
    "    # Fix labels for retweets\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"FALSE\", False)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].replace(\"TRUE\", True)\n",
    "    df[\"is_retweet\"] = df[\"is_retweet\"].astype(\"bool\")\n",
    "\n",
    "    # Remove tweets from parties that are not in the Bundestag and/or retweets\n",
    "    print(\n",
    "        f\"The dataset contains {len(df.loc[(df['is_retweet'] == True) | (df['text'].str.startswith('RT'))])} retweets...\"\n",
    "    )\n",
    "    df = df.loc[(df[\"party\"] != \"Parteilos\") & (df[\"is_retweet\"] == False) & (~df[\"text\"].str.startswith(\"RT\"))]\n",
    "\n",
    "    # Encode party and gender\n",
    "    df[\"party\"] = df[\"party\"].map(party_encoding).astype(\"int8\")\n",
    "    df[\"gender\"] = df[\"gender\"].map(gender_encoding).astype(\"int8\")\n",
    "\n",
    "    # Apply cleaning pipeline\n",
    "    df[\"clean_text\"] = df[\"text\"].progress_apply(clean.clean_text).astype(\"string[pyarrow]\")\n",
    "    df[\"lemma_text\"] = df[\"clean_text\"].progress_apply(clean.lemma_text).astype(\"string[pyarrow]\")\n",
    "    df[\"filter_text\"] = df[\"lemma_text\"].progress_apply(clean.filter_text).astype(\"string[pyarrow]\")\n",
    "\n",
    "    # Count the number of words and tokens in the tweet\n",
    "    df[\"init_word_count\"] = df[\"text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"init_symbol_count\"] = df[\"text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "    df[\"clean_word_count\"] = df[\"clean_text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"clean_symbol_count\"] = df[\"clean_text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "    df[\"filter_word_count\"] = df[\"filter_text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "    df[\"filter_symbol_count\"] = df[\"filter_text\"].progress_apply(lambda x: len(x)).astype(\"int16\")\n",
    "\n",
    "    # Filter out tweets that are too short\n",
    "    print(\n",
    "        f\"Found {len(df.loc[df['filter_word_count'] < min_word_count])} tweets with less than {min_word_count} words...\"\n",
    "    )\n",
    "    df = df.loc[df[\"filter_word_count\"] >= min_word_count]\n",
    "\n",
    "    # Calculate the sentiment of the tweets\n",
    "    if SENTIMENT_ANALYSIS:\n",
    "        df[\"sentiment\"] = df[\"clean_text\"].progress_apply(sentiment.predict_sentiment).astype(\"category\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either load the cached data or process the raw tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = []\n",
    "\n",
    "if SENTIMENT_ANALYSIS:\n",
    "    suffix.append(\"sent\")\n",
    "\n",
    "if SAMPLE_SIZE is None:\n",
    "    suffix.append(\"full\")\n",
    "elif SAMPLE_SIZE <= 25000:\n",
    "    suffix.append(\"sm\")\n",
    "elif SAMPLE_SIZE <= 50000:\n",
    "    suffix.append(\"md\")\n",
    "elif SAMPLE_SIZE <= 100000:\n",
    "    suffix.append(\"lg\")\n",
    "\n",
    "file_path = f\"prep_tweets_{'_'.join(suffix)}.parquet\"\n",
    "\n",
    "if FAST_MODE and load.check_file_exists(file_path):\n",
    "    df_prep = load.load_dataframe(file_path)\n",
    "else:\n",
    "    df_prep = prep_pipeline(df_base.sample(SAMPLE_SIZE, random_state=42).copy() if SAMPLE_SIZE else df_base.copy())\n",
    "    load.save_dataframe(df_prep, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test purposes\n",
    "df_prep[\"init_word_count\"] = df_prep[\"text\"].progress_apply(lambda x: len(x.split())).astype(\"int16\")\n",
    "df_prep[\"init_symbol_count\"] = df_prep[\"text\"].progress_apply(lambda x: len(x)).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\n",
    "    list(itertools.chain.from_iterable(df_prep[\"filter_text\"].str.split().apply(lambda x: list(ngrams(x, 3)))))\n",
    ").most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.info(verbose=True, memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_prep.copy()\n",
    "df_plot[\"party\"] = df_plot[\"party\"].map({value: key for key, value in party_encoding.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.party_count(df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sentiment\" in df_plot.columns:\n",
    "    plot.sentiment(df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.word_count(df_plot, column=\"init_word_count\", x_lim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.word_count(df_plot, x_lim=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.gender(df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.user_count(df_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_prep.select_dtypes(exclude=[\"object\", \"category\", \"datetime64[ns]\", \"bool\"]).corr(numeric_only=True)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "mask[np.diag_indices_from(mask)] = False\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "corr_plot = sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    "    annot=True,\n",
    "    annot_kws={\"fontsize\": 10},\n",
    "    fmt=\".2f\",\n",
    "    ax=ax,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7abe641ef5c60e7b2b79f06dad82c6b1ae6b3c4f8500bc012ee8285dc22561c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
